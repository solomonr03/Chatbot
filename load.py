# -*- coding: utf-8 -*-
"""load.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19khqjk0X7HO07UV5BiVRDcqFsNCOlDN7
"""

#SOURCE: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chatbot
# and put in a ``data/`` directory under the current directory.
#
# After that, letâ€™s import some necessities.
#

import torch
from torch.jit import script, trace
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import csv
import random
import re
import os
import unicodedata
import codecs
from io import open
import itertools
import math
import json
from google.colab import drive
drive.mount('/content/gdrive')


USE_CUDA = torch.cuda.is_available()
device = torch.device("cuda" if USE_CUDA else "cpu")

corpus_name = "movie-corpus"
corpus = os.path.join("/content/gdrive/MyDrive/data", corpus_name)

def printLines(file, n=6):
    with open(file, 'rb') as datafile:
        lines = datafile.readlines()
    for line in lines[:n]:
        print(line)

printLines(os.path.join(corpus, "utterances.jsonl"))

def lines_and_conversations(fileName):
  lines = {}
  conversations = {}
  with open(fileName, 'r', encoding='iso-8859-1') as f:
    #Parse the loaded data and create our line and conversation dictionaries
    for line in f:
      lineJson = json.loads(line)
      lineObj = {}
      lineObj["lineID"] = lineJson["id"]
      lineObj["charID"] = lineJson["speaker"]
      lineObj["sen"] = lineJson["text"]
      lines[lineJson["id"]] = lineObj

      if lineJson["conversation_id"] not in conversations:
        convObj = {}
        convObj["convID"] = lineJson["conversation_id"]
        convObj["movieID"] = lineJson["meta"]["movie_id"]
        convObj["lines"] = [lineObj]
      else:
        convObj = conversations[lineJson["conversation_id"]]
        convObj["lines"].insert(0,lineObj)
      conversations[lineJson["conversation_id"]] = convObj
  return line, conversations

def get_sentences_pairs(conversations):
  pairs = []
  for conv in conversations.values():
    for i in range(len(conv["lines"])-1):
      input = conv["lines"][i]["sen"].strip()
      target = conv["lines"][i+1]["sen"].strip()
      if input and target:
        pairs.append([input, target])
  return pairs

datafile = os.path.join(corpus, "formatted_lines.txt")

delimiter = str(codecs.decode('\t', "unicode_escape"))

lines = {}
conversations = {}

lines, conversations = lines_and_conversations(os.path.join(corpus, "utterances.jsonl"))

with open(datafile, 'w', encoding='utf-8') as outputfile:
  writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\n')
  for pair in get_sentences_pairs(conversations):
    writer.writerow(pair)

printLines(datafile)

#Create a vocabulary that maps words to a discrete numerical space

PAD_token = 0 # Padding to maintain shape
SOS_token = 1 # Start of sentence token
EOS_token = 2 # End of sentence token

class Voc:
  def __init__(self, name):
    self.name = name
    self.trimmed = False
    self.word2index = {}
    self.word2count = {}
    self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}
    self.num_words = 3

  def addWord(self, word):
    if word not in self.word2index:
      self.word2index[word] = self.num_words
      self.word2count[word] = 1
      self.index2word[self.num_words] = word
      self.num_words += 1
    else:
      self.word2count[word] += 1

  def addSentence(self, sen):
    for word in sen.split(' '):
      self.addWord(word)

  def trim(self, min_count):
    if self.trimmed:
      return

    self.trimmed = True

    keep_words = []

    for k, v in self.word2count.items():
      if v >= min_count:
        keep_words.append(k)

    print('keep_words {} / {} = {:.4f}'.format(len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))

    self.word2index = {}
    self.word2count = {}
    self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}
    self.num_words = 3

    for word in keep_words:
      self.addWord(word)

#Convert unicode strings to ascii and normalize sentences
#Filter long sentences to help with training

MAX_LEN = 10

def unicode2ascii(s):
  return ''.join(
      c for c in unicodedata.normalize('NFD', s)
      if unicodedata.category(c)
  )

def normalizeString(s):
  s = unicode2ascii(s.lower().strip())
  s = re.sub(r"([.!>])", r"\1", s)
  s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
  s = re.sub(r"\s+", r" ", s).strip()
  return s

def readVocs(datafile, corpus_name):
  lines = open(datafile, encoding='utf-8').read().strip().split('\n')
  pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]
  voc = Voc(corpus_name)
  return voc, pairs

def filterPair(p):
  return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN

def filterPairs(pairs):
  return [pair for pair in pairs if filterPair(pair)]

def loadPrepareData(corpus, corpus_name, datfile, save_dir):
  voc, pairs = readVocs(datafile, corpus_name)
  pairs = filterPairs(pairs)
  for pair in pairs:
    voc.addSentence(pair[0])
    voc.addSentence(pair[1])
  return voc, pairs

save_dir = os.path.join("/content/gdrive/MyDrive/data", "save")
voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)

#Trim rarely used words out of vocabulary for faster convergence during training
#Decrease feature space to help the difficulty of the function the model will learn


MIN_COUNT = 3

def trimRareWords(voc, pairs, MIN_COUNT):
  voc.trim(MIN_COUNT)
  keep_pairs = []
  for pair in pairs:
    input_sentence = pair[0]
    output_sentence = pair[1]
    keep_input = True
    keep_output = True
    for word in input_sentence.split(' '):
      if word not in voc.word2index:
        keep_input = False
        break
    for word in output_sentence.split(' '):
      if word not in voc.word2index:
        keep_output = False
        break
    if keep_input and keep_output:
      keep_pairs.append(pair)

  print("Trimmed from {} pairs to {}, {:.4f} of total".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))
  return keep_pairs
pairs = trimRareWords(voc, pairs, MIN_COUNT)

#Prepare data for model by putting it into minibatches

def indexesFromSentence(voc, sentence):
  return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]

def zeroPadding(l, fillvalue=PAD_token):
  return list(itertools.zip_longest(*l, fillvalue=fillvalue))

def binaryMatrix(l, value=PAD_token):
  m = []
  for i, seq in enumerate(l):
    m.append([])
    for token in seq:
      if token == PAD_token:
        m[i].append(0)
      else:
        m[i].append(1)
  return m

def inputVar(l, voc):
  indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]
  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])
  padList = zeroPadding(indexes_batch)
  padVar = torch.LongTensor(padList)
  return padVar, lengths

def outputVar(l, voc):
  indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]
  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])
  max_target_len = max([len(indexes) for indexes in indexes_batch])
  padList = zeroPadding(indexes_batch)
  padVar = torch.LongTensor(padList)
  return padVar, lengths, max_target_len

def batch2TrainData(voc, pair_batch):
  pair_batch.sort(key=lambda x:len(x[0].split(" ")), reverse=True)
  input_batch, output_batch = [], []
  for pair in pair_batch:
    input_batch.append(pair[0])
    output_batch.append(pair[1])
  inp, lengths = inputVar(input_batch, voc)
  output, mask, max_target_len = outputVar(output_batch, voc)
  return inp, lengths, output, mask, max_target_len

small_batch_size = 5
batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])
input_variable, lengths, target_variable, mask, max_target_len = batches

print("input_variable:", input_variable)
print("lengths:", lengths)
print("target_variable:", target_variable)
print("mask:", mask)
print("max_target_len:", max_target_len)